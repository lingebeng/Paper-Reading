# Transformer 自回归效率低下示意图

## 1. 自回归生成过程

```
生成序列: "The cat sits on"

Step 1: 生成 "The"
┌─────────────────────────────────┐
│  Input: [BOS]                   │
│  ↓                               │
│  Transformer (全部层计算)        │
│  ↓                               │
│  Output: "The"                  │
└─────────────────────────────────┘
计算量: N层 × d维度 × 1个token

Step 2: 生成 "cat"
┌─────────────────────────────────┐
│  Input: [BOS] The               │
│  ↓                               │
│  Transformer (全部层计算)        │
│  ↓                               │
│  Output: "cat"                  │
└─────────────────────────────────┘
计算量: N层 × d维度 × 2个token

Step 3: 生成 "sits"
┌─────────────────────────────────┐
│  Input: [BOS] The cat           │
│  ↓                               │
│  Transformer (全部层计算)        │
│  ↓                               │
│  Output: "sits"                 │
└─────────────────────────────────┘
计算量: N层 × d维度 × 3个token

Step 4: 生成 "on"
┌─────────────────────────────────┐
│  Input: [BOS] The cat sits      │
│  ↓                               │
│  Transformer (全部层计算)        │
│  ↓                               │
│  Output: "on"                   │
└─────────────────────────────────┘
计算量: N层 × d维度 × 4个token

总计算量: N × d × (1+2+3+4+...+T) = N × d × T(T+1)/2 ≈ O(T²)
```

## 2. 效率低下的核心原因

### 问题1: 重复计算历史Token
```
Step 2 计算:
┌──────────┬──────────┐
│ [BOS]    │   The    │
└──────────┴──────────┘
    ↓          ↓
   重复      新计算

Step 3 计算:
┌──────────┬──────────┬──────────┐
│ [BOS]    │   The    │   cat    │
└──────────┴──────────┴──────────┘
    ↓          ↓          ↓
   重复       重复       新计算

Step 4 计算:
┌──────────┬──────────┬──────────┬──────────┐
│ [BOS]    │   The    │   cat    │  sits    │
└──────────┴──────────┴──────────┴──────────┘
    ↓          ↓          ↓          ↓
   重复       重复       重复       新计算
```

**解释**: 每一步都要重新计算所有历史token的表示，造成大量冗余计算。

### 问题2: KV Cache 的内存开销

```
每层需要缓存的 KV:
┌────────────────────────────────────────────┐
│ Layer 1: K₁[T×d], V₁[T×d]                 │
├────────────────────────────────────────────┤
│ Layer 2: K₂[T×d], V₂[T×d]                 │
├────────────────────────────────────────────┤
│ Layer 3: K₃[T×d], V₃[T×d]                 │
├────────────────────────────────────────────┤
│        ...                                 │
├────────────────────────────────────────────┤
│ Layer N: Kₙ[T×d], Vₙ[T×d]                 │
└────────────────────────────────────────────┘

总内存: 2 × N × T × d × sizeof(float16)

示例 (LLaMA-7B, T=2048):
- N = 32 层
- d = 4096 维度
- 内存 = 2 × 32 × 2048 × 4096 × 2 bytes
       ≈ 1 GB per request
```

## 3. 注意力计算的序列依赖

```
生成第t个token时的注意力计算:

Query(t): 当前token的查询向量
Keys(1:t): 所有历史token的键向量

Attention(Q, K, V):
┌─────────────────────────────────────────┐
│  Q_t · [K₁, K₂, K₃, ..., K_{t-1}, K_t] │
│   ↓                                     │
│  Softmax([s₁, s₂, s₃, ..., s_t])       │
│   ↓                                     │
│  Σ score_i × V_i                        │
└─────────────────────────────────────────┘

必须等待 token t-1 生成完成才能开始计算 token t
⇒ 无法并行化生成过程
```

## 4. 对比: 理想并行 vs 实际串行

### 理想情况 (训练时 - Teacher Forcing):
```
Input:  [BOS] The  cat  sits  on
         ↓     ↓    ↓    ↓    ↓
    ┌────────────────────────────┐
    │    Transformer (1次前向)    │
    └────────────────────────────┘
         ↓     ↓    ↓    ↓    ↓
Output:  The  cat  sits  on  mat

计算量: O(T) - 单次前向传播
时间: T个token并行处理
```

### 实际情况 (推理时 - 自回归):
```
Input: [BOS] → Output: The
       ↓ (等待)
Input: [BOS] The → Output: cat
       ↓ (等待)
Input: [BOS] The cat → Output: sits
       ↓ (等待)
Input: [BOS] The cat sits → Output: on
       ↓ (等待)
Input: [BOS] The cat sits on → Output: mat

计算量: O(T²) - T次前向传播
时间: T次串行计算 (无法并行)
```

## 5. GPU 利用率问题

```
批处理推理时的 GPU 利用率:

Batch内不同请求的进度:
┌─────────────────────────────────────────────┐
│ Request 1: ████████████████████ (20 tokens) │
│ Request 2: ██████████ (10 tokens)           │
│ Request 3: ████████████████████████ (24)    │
│ Request 4: ███████ (7 tokens)               │
└─────────────────────────────────────────────┘

问题:
- 不同长度导致计算不均衡
- 短序列完成后GPU空闲
- Padding 浪费计算资源
- 内存碎片化严重
```

## 6. 延迟分解

```
单个 Token 的生成延迟:

┌─────────────────────────────────────────┐
│ 1. 准备输入 (拼接历史)      ~0.1ms     │
├─────────────────────────────────────────┤
│ 2. 前向传播 (N层Transformer) ~50ms     │
│    - Attention计算          30ms       │
│    - FFN计算                15ms       │
│    - Layer Norm等           5ms        │
├─────────────────────────────────────────┤
│ 3. Sampling (温度/top-p)    ~0.5ms     │
└─────────────────────────────────────────┘
总计: ~50.6ms per token

生成100个token: 50.6ms × 100 = 5.06秒
```

## 7. 效率对比表

| 指标 | 训练(并行) | 推理(自回归) | 效率差距 |
|------|-----------|-------------|---------|
| 计算复杂度 | O(T) | O(T²) | T倍 |
| 内存占用 | O(batch×T) | O(batch×T×N×2) | 2N倍 |
| GPU利用率 | 80-90% | 20-40% | 2-4倍 |
| 吞吐量 | 1000+ tok/s | 10-50 tok/s | 20-100倍 |
| 可并行性 | 完全并行 | 串行依赖 | - |

## 8. 改进方向

```
现有优化方案:

1. KV Cache (减少重复计算)
   ┌─────────────────────┐
   │ 缓存历史 K, V        │
   │ 只计算新token        │
   └─────────────────────┘
   效果: O(T²) → O(T)

2. PagedAttention (优化内存)
   ┌─────────────────────┐
   │ 分页管理 KV Cache    │
   │ 减少内存碎片         │
   └─────────────────────┘
   效果: 内存利用率 ↑2-4倍

3. Speculative Decoding (推测执行)
   ┌─────────────────────┐
   │ 小模型预测多个token  │
   │ 大模型批量验证       │
   └─────────────────────┘
   效果: 速度提升2-3倍

4. FlashAttention (优化计算)
   ┌─────────────────────┐
   │ 融合kernel操作       │
   │ 减少内存访问         │
   └─────────────────────┘
   效果: 计算速度 ↑2-4倍
```

## 核心结论

**Transformer 自回归效率低的根本原因**:
1. **序列依赖**: 必须串行生成，无法并行
2. **重复计算**: 每步重新计算历史token (KV Cache可缓解)
3. **内存墙**: 大量KV Cache占用显存
4. **GPU利用不足**: 单token生成时计算量小，GPU空闲

**时间复杂度**: O(T²) - 生成T个token需要T次前向传播
**空间复杂度**: O(N×T×d) - N层需要缓存T个token的KV

这也是为什么需要 vLLM、PagedAttention 等优化技术的原因！
