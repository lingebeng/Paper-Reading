# vLLM: Efficient Memory Management for Large Language Model Serving with PagedAttention

> **è®ºæ–‡ä¿¡æ¯**
> - æ ‡é¢˜: Efficient Memory Management for Large Language Model Serving with PagedAttention
> - ä½œè€…: Woosuk Kwon, Zhuohan Li, et al. (UC Berkeley, Stanford)
> - ä¼šè®®: SOSP 2023
> - é“¾æ¥: https://github.com/vllm-project/vllm

---

## ç›®å½•

- [ä¸€ã€æ ¸å¿ƒé—®é¢˜ä¸åŠ¨æœº](#ä¸€æ ¸å¿ƒé—®é¢˜ä¸åŠ¨æœº)
- [äºŒã€PagedAttention æ ¸å¿ƒåˆ›æ–°](#äºŒpagedattention-æ ¸å¿ƒåˆ›æ–°)
- [ä¸‰ã€ç³»ç»Ÿæ¶æ„ä¸å…³é”®æŠ€æœ¯](#ä¸‰ç³»ç»Ÿæ¶æ„ä¸å…³é”®æŠ€æœ¯)
- [å››ã€å®éªŒç»“æœä¸æ€§èƒ½åˆ†æ](#å››å®éªŒç»“æœä¸æ€§èƒ½åˆ†æ)
- [äº”ã€å…³é”®å®ç°ç»†èŠ‚](#äº”å…³é”®å®ç°ç»†èŠ‚)
- [å…­ã€æ ¸å¿ƒè´¡çŒ®ä¸å½±å“](#å…­æ ¸å¿ƒè´¡çŒ®ä¸å½±å“)
- [ä¸ƒã€å±€é™ä¸æœªæ¥æ–¹å‘](#ä¸ƒå±€é™ä¸æœªæ¥æ–¹å‘)

---

## ä¸€ã€æ ¸å¿ƒé—®é¢˜ä¸åŠ¨æœº

### 1.1 LLM æ¨ç†é¢ä¸´çš„æŒ‘æˆ˜

è¿™ç¯‡è®ºæ–‡è§£å†³çš„æ ¸å¿ƒé—®é¢˜æ˜¯ï¼š**å¤§æ¨¡å‹æ¨ç†æ—¶ KV Cache çš„å†…å­˜ç®¡ç†æ•ˆç‡æä½**ã€‚

#### å†…å­˜åˆ†å¸ƒç°çŠ¶

ä»¥ OPT-13B æ¨¡å‹åœ¨ NVIDIA A100 (40GB) ä¸Šè¿è¡Œä¸ºä¾‹ï¼š

| å†…å­˜ç»„æˆ | å æ¯” | ç‰¹ç‚¹ |
|---------|------|------|
| æ¨¡å‹å‚æ•° | ~65% (26GB) | é™æ€å›ºå®š |
| **KV Cache** | **~30%** | **åŠ¨æ€å˜åŒ–** |
| æ¿€æ´»å€¼ | ~5% | ä¸´æ—¶ä½¿ç”¨ |

#### KV Cache çš„ç‰¹ç‚¹

1. **ç©ºé—´å ç”¨å·¨å¤§**
   - å•ä¸ª token çš„ KV Cache: 800 KB
   - è®¡ç®—å…¬å¼: `2 (key+value) Ã— 5120 (hidden) Ã— 40 (layers) Ã— 2 (FP16)`
   - å•ä¸ªè¯·æ±‚æœ€å¤§å¯è¾¾ 1.6 GB (2048 tokens)

2. **åŠ¨æ€ç‰¹æ€§**
   - é•¿åº¦éšç”Ÿæˆè¿‡ç¨‹åŠ¨æ€å¢é•¿
   - ç”Ÿå‘½å‘¨æœŸä¸å¯é¢„çŸ¥
   - ä¸ä¼ ç»Ÿæ·±åº¦å­¦ä¹  tensor æœ‰æœ¬è´¨åŒºåˆ«

3. **æ€§èƒ½ç“¶é¢ˆ**
   - Autoregressive ç”Ÿæˆæ˜¯ memory-bound æ“ä½œ
   - GPU è®¡ç®—èƒ½åŠ›æœªè¢«å……åˆ†åˆ©ç”¨
   - å†…å­˜å®¹é‡é™åˆ¶äº† batch size

### 1.2 ç°æœ‰ç³»ç»Ÿçš„å†…å­˜æµªè´¹

è®ºæ–‡é€šè¿‡ç²¾ç¡®æµ‹é‡å‘ç°ï¼Œç°æœ‰ç³»ç»Ÿï¼ˆFasterTransformer, Orcaï¼‰çš„å†…å­˜åˆ©ç”¨ç‡æä½ã€‚

#### å†…å­˜æµªè´¹çš„ä¸‰ç§å½¢å¼

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Reserved Memory (é¢„ç•™å†…å­˜)                              â”‚
â”‚  - ä¸ºæœªæ¥çš„ token é¢„ç•™ç©ºé—´                               â”‚
â”‚  - åœ¨æ•´ä¸ªè¯·æ±‚ç”Ÿå‘½å‘¨æœŸä¸­è¢«å ç”¨                             â”‚
â”‚  - æ— æ³•è¢«å…¶ä»–è¯·æ±‚ä½¿ç”¨                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Internal Fragmentation (å†…éƒ¨ç¢ç‰‡)                       â”‚
â”‚  - é¢„åˆ†é…äº†æœ€å¤§é•¿åº¦ï¼Œä½†å®é™…ä½¿ç”¨æ›´çŸ­                       â”‚
â”‚  - åªæœ‰åœ¨è¯·æ±‚å®Œæˆåæ‰çŸ¥é“æµªè´¹äº†å¤šå°‘                       â”‚
â”‚  - ä¾‹ï¼šé¢„åˆ†é… 2048ï¼Œå®é™…åªç”¨ 100                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  External Fragmentation (å¤–éƒ¨ç¢ç‰‡)                       â”‚
â”‚  - ä¸åŒè¯·æ±‚çš„æœ€å¤§é•¿åº¦ä¸åŒ                                â”‚
â”‚  - Buddy allocator å¯¼è‡´çš„ç¢ç‰‡                           â”‚
â”‚  - æ— æ³•è¢«ä»»ä½•è¯·æ±‚ä½¿ç”¨                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Token States (å®é™…æœ‰æ•ˆæ•°æ®) âœ“                          â”‚
â”‚  - çœŸæ­£å­˜å‚¨ KV Cache çš„ç©ºé—´                             â”‚
â”‚  - åœ¨ç°æœ‰ç³»ç»Ÿä¸­ä»…å  20.4% - 38.2%                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### å®æµ‹æ•°æ®ï¼ˆå›¾ 2ï¼‰

| ç³»ç»Ÿ | æœ‰æ•ˆå†…å­˜ | é¢„ç•™ | å†…éƒ¨ç¢ç‰‡ | å¤–éƒ¨ç¢ç‰‡ |
|-----|---------|------|---------|---------|
| **Orca (Max)** | 20.4% | 13.3% | 26.8% | 41.6% |
| **Orca (Pow2)** | 38.2% | 17.9% | 13.6% | 25.2% |
| **Orca (Oracle)** | 57.3% | 8.9% | - | 36.6% |
| **vLLM** | **96.3%** | - | 3.7% | - |

> ğŸ’¡ **å…³é”®å‘ç°**: ç°æœ‰ç³»ç»Ÿæµªè´¹äº† 60%-80% çš„ KV Cache å†…å­˜ï¼

### 1.3 å¤æ‚è§£ç ç®—æ³•çš„æŒ‘æˆ˜

ç°ä»£ LLM æœåŠ¡éœ€è¦æ”¯æŒå¤šç§è§£ç ç®—æ³•ï¼š

1. **Parallel Sampling** (å¹¶è¡Œé‡‡æ ·)
   - ç”¨ä¾‹ï¼šä»£ç è¡¥å…¨ï¼ˆGitHub Copilotï¼‰
   - ç‰¹ç‚¹ï¼šå…±äº« prompt çš„ KV Cache
   - å†…å­˜å…±äº«æ¯”ä¾‹ï¼š12% (å®éªŒæ•°æ®)

2. **Beam Search** (æŸæœç´¢)
   - ç”¨ä¾‹ï¼šæœºå™¨ç¿»è¯‘
   - ç‰¹ç‚¹ï¼šåŠ¨æ€å…±äº«ï¼Œä¸åŒå€™é€‰ä¹‹é—´éƒ¨åˆ†å…±äº«
   - å†…å­˜å…±äº«æ¯”ä¾‹ï¼š37%-66% (å®éªŒæ•°æ®)

3. **Shared Prefix** (å…±äº«å‰ç¼€)
   - ç”¨ä¾‹ï¼šFew-shot learning
   - ç‰¹ç‚¹ï¼šå¤šä¸ªè¯·æ±‚å…±äº« system prompt
   - å†…å­˜å…±äº«æ¯”ä¾‹ï¼šå–å†³äº prefix é•¿åº¦

**ç°æœ‰ç³»ç»Ÿçš„é—®é¢˜**ï¼š
- æ— æ³•å®ç°è·¨åºåˆ—çš„å†…å­˜å…±äº«
- éœ€è¦é¢‘ç¹å¤åˆ¶ KV Cache
- é™åˆ¶äº†æ‰¹å¤„ç†çš„çµæ´»æ€§

---

## äºŒã€PagedAttention æ ¸å¿ƒåˆ›æ–°

### 2.1 è®¾è®¡çµæ„Ÿï¼šæ“ä½œç³»ç»Ÿçš„è™šæ‹Ÿå†…å­˜

vLLM çš„æ ¸å¿ƒåˆ›æ–°æ˜¯å°† **æ“ä½œç³»ç»Ÿçš„è™šæ‹Ÿå†…å­˜å’Œåˆ†é¡µæŠ€æœ¯** åº”ç”¨åˆ° KV Cache ç®¡ç†ã€‚

#### æ¦‚å¿µæ˜ å°„

| æ“ä½œç³»ç»Ÿæ¦‚å¿µ | PagedAttention å¯¹åº” | è¯´æ˜ |
|------------|-------------------|------|
| **è¿›ç¨‹ (Process)** | **è¯·æ±‚ (Request)** | ç‹¬ç«‹çš„æ‰§è¡Œå•å…ƒ |
| **è™šæ‹Ÿé¡µ (Virtual Page)** | **é€»è¾‘ KV Block** | è¿ç»­çš„é€»è¾‘åœ°å€ç©ºé—´ |
| **ç‰©ç†é¡µ (Physical Page)** | **ç‰©ç† KV Block** | å®é™…çš„ GPU å†…å­˜å— |
| **å­—èŠ‚ (Byte)** | **Token** | æœ€å°çš„æ•°æ®å•å…ƒ |
| **é¡µè¡¨ (Page Table)** | **Block Table** | é€»è¾‘åˆ°ç‰©ç†çš„æ˜ å°„ |
| **è™šæ‹Ÿå†…å­˜ (Virtual Memory)** | **KV Cache Manager** | ç»Ÿä¸€çš„å†…å­˜ç®¡ç†æ¥å£ |

#### å…³é”®ä¼˜åŠ¿

```
ä¼ ç»Ÿæ–¹æ³•ï¼ˆè¿ç»­å†…å­˜ï¼‰:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Token 1 ... Token 2048 (é¢„åˆ†é…)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
é—®é¢˜: æµªè´¹ã€æ— æ³•å…±äº«

PagedAttentionï¼ˆåˆ†å—å†…å­˜ï¼‰:
â”Œâ”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”
â”‚Block0â”‚ -> â”‚Block1â”‚ -> â”‚Block2â”‚  (æŒ‰éœ€åˆ†é…)
â””â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”˜
  â†“           â†“           â†“
 ç‰©ç†7        ç‰©ç†1        ç‰©ç†3   (éè¿ç»­)
ä¼˜åŠ¿: é›¶æµªè´¹ã€çµæ´»å…±äº«
```

### 2.2 PagedAttention ç®—æ³•

#### ä¼ ç»Ÿ Attention å…¬å¼

å¯¹äºè¾“å…¥åºåˆ—ä¸­çš„ç¬¬ `i` ä¸ª tokenï¼š

$a_{i,j} = exp(q_i^T Â· k_j / âˆšd) / Î£_{t=1}^i exp(q_i^T Â· k_t / âˆšd)$
$o_i = Î£_{j=1}^i a_ij Â· v_j$

å…¶ä¸­ï¼š
- `q_i`: æŸ¥è¯¢å‘é‡
- `k_j, v_j`: é”®å’Œå€¼å‘é‡
- `d`: ç»´åº¦
- `a_ij`: attention score
- `o_i`: è¾“å‡º

**çº¦æŸ**: æ‰€æœ‰ `k_j` å’Œ `v_j` å¿…é¡»å­˜å‚¨åœ¨è¿ç»­å†…å­˜ä¸­

#### PagedAttention æ”¹è¿›

å°† KV Cache åˆ†æˆå›ºå®šå¤§å°çš„å—ï¼Œæ¯å—åŒ…å« `B` ä¸ª tokenï¼š

```
K_j = (k_{(j-1)B+1}, ..., k_{jB})  # ç¬¬ j ä¸ª key block
V_j = (v_{(j-1)B+1}, ..., v_{jB})  # ç¬¬ j ä¸ª value block
```

å—çº§ Attention è®¡ç®—ï¼š

```
A_ij = exp(q_i^T Â· K_j / âˆšd) / Î£_{t=1}^âŒˆi/BâŒ‰ exp(q_i^T Â· K_t Â· 1 / âˆšd)

o_i = Î£_{j=1}^âŒˆi/BâŒ‰ V_j Â· A_ij^T
```

å…¶ä¸­ `A_ij` æ˜¯ä¸€ä¸ªè¡Œå‘é‡ï¼ŒåŒ…å«å¯¹ç¬¬ `j` å—çš„æ‰€æœ‰ attention scoresã€‚

#### ç®—æ³•æµç¨‹ç¤ºä¾‹

```
Prompt: "Four score and seven years ago our fathers brought forth"

é€»è¾‘å—åˆ†å¸ƒ:
Block 0: [Four, score, and, seven]      -> ç‰©ç†å— 7
Block 1: [years, ago, our, fathers]     -> ç‰©ç†å— 1
Block 2: [brought, forth]               -> ç‰©ç†å— 3

æŸ¥è¯¢ token: "forth"
è®¡ç®—æ­¥éª¤:
1. è¯»å–ç‰©ç†å— 7 -> è®¡ç®— A_{i,0}
2. è¯»å–ç‰©ç†å— 1 -> è®¡ç®— A_{i,1}
3. è¯»å–ç‰©ç†å— 3 -> è®¡ç®— A_{i,2}
4. åˆå¹¶: o_i = V_0Â·A_{i,0}^T + V_1Â·A_{i,1}^T + V_2Â·A_{i,2}^T
```

### 2.3 Block Table æœºåˆ¶

#### æ•°æ®ç»“æ„

æ¯ä¸ªè¯·æ±‚ç»´æŠ¤ä¸€ä¸ª Block Tableï¼š

```python
class BlockTable:
    entries: List[BlockEntry]

class BlockEntry:
    logical_block_id: int      # é€»è¾‘å—å·
    physical_block_id: int     # ç‰©ç†å—å·
    num_filled: int            # å·²å¡«å……çš„ token æ•°
    ref_count: int             # å¼•ç”¨è®¡æ•°ï¼ˆç”¨äºå…±äº«ï¼‰
```

#### ç¤ºä¾‹

```
Request A: "Four score and seven years ago our fathers brought"

Block Table:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Logical â”‚ Physical â”‚ # Filled  â”‚ Ref Count â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    0    â”‚    7     â”‚     4     â”‚     1     â”‚
â”‚    1    â”‚    1     â”‚     3     â”‚     1     â”‚
â”‚    2    â”‚    3     â”‚     1     â”‚     1     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ç”Ÿæˆè¿‡ç¨‹:
1. Prefill: å¤„ç† prompt -> å¡«å…… Block 0, 1
2. Decode step 1: ç”Ÿæˆ "fathers" -> å¡«å……åˆ° Block 1
3. Decode step 2: ç”Ÿæˆ "brought" -> åˆ†é… Block 2
```

---

## ä¸‰ã€ç³»ç»Ÿæ¶æ„ä¸å…³é”®æŠ€æœ¯

### 3.1 vLLM ç³»ç»Ÿæ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Centralized Scheduler                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚            KV Cache Manager                       â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â”‚ Block Tables   â”‚  Mapping: Logical -> Phys  â”‚ â”‚  â”‚
â”‚  â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚  â”‚
â”‚  â”‚  â”‚ CPU Allocator  â”‚  Swap space in CPU RAM     â”‚ â”‚  â”‚
â”‚  â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚  â”‚
â”‚  â”‚  â”‚ GPU Allocator  â”‚  Physical blocks on GPU    â”‚ â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚              â”‚             â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   Worker 0    â”‚  â”‚  Worker 1   â”‚  â”‚ Worker N-1â”‚
         â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
         â”‚ â”‚Model      â”‚ â”‚  â”‚ â”‚Model    â”‚ â”‚  â”‚ â”‚Model   â”‚â”‚
         â”‚ â”‚Shard 0    â”‚ â”‚  â”‚ â”‚Shard 1  â”‚ â”‚  â”‚ â”‚Shard Nâ”‚â”‚
         â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
         â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
         â”‚ â”‚Cache      â”‚ â”‚  â”‚ â”‚Cache    â”‚ â”‚  â”‚ â”‚Cache   â”‚â”‚
         â”‚ â”‚Engine     â”‚ â”‚  â”‚ â”‚Engine   â”‚ â”‚  â”‚ â”‚Engine  â”‚â”‚
         â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### ç»„ä»¶è¯´æ˜

1. **Centralized Scheduler**
   - ç®¡ç†æ‰€æœ‰è¯·æ±‚çš„è°ƒåº¦
   - åè°ƒåˆ†å¸ƒå¼ GPU workers
   - å®ç° FCFS è°ƒåº¦ç­–ç•¥

2. **KV Cache Manager**
   - ç»´æŠ¤æ‰€æœ‰è¯·æ±‚çš„ Block Tables
   - ç®¡ç† CPU å’Œ GPU çš„ç‰©ç†å—åˆ†é…
   - å¤„ç†å—çš„åˆ†é…ã€é‡Šæ”¾ã€å…±äº«

3. **GPU Workers**
   - æ‰§è¡Œæ¨¡å‹æ¨ç†
   - é€šè¿‡ Block Table è®¿é—® KV Cache
   - æ”¯æŒå¼ é‡å¹¶è¡Œï¼ˆMegatron-LM é£æ ¼ï¼‰

### 3.2 æ ¸å¿ƒæŠ€æœ¯è¯¦è§£

#### æŠ€æœ¯ 1: åŠ¨æ€å†…å­˜åˆ†é…

**æŒ‰éœ€åˆ†é…ç­–ç•¥**ï¼š

```python
# ä¼ªä»£ç 
def process_request(prompt, max_tokens):
    # 1. Prefill é˜¶æ®µï¼šåªåˆ†é… prompt éœ€è¦çš„å—
    prompt_length = len(prompt)
    num_blocks_needed = ceil(prompt_length / BLOCK_SIZE)

    logical_blocks = [0, 1, ..., num_blocks_needed-1]
    physical_blocks = allocator.allocate(num_blocks_needed)

    # å»ºç«‹æ˜ å°„
    block_table[request_id] = {
        logical_blocks[i]: physical_blocks[i]
        for i in range(num_blocks_needed)
    }

    # 2. Decode é˜¶æ®µï¼šæŒ‰éœ€åˆ†é…æ–°å—
    for step in range(max_tokens):
        token = generate_token(request_id)

        last_block = block_table[request_id][-1]
        if last_block.is_full():
            # åˆ†é…æ–°çš„ç‰©ç†å—
            new_physical = allocator.allocate(1)
            block_table[request_id].append(new_physical)
```

**ä¼˜åŠ¿**ï¼š
- é›¶é¢„ç•™å†…å­˜æµªè´¹
- å†…éƒ¨ç¢ç‰‡ä»…é™äºæœ€åä¸€ä¸ªå—
- å¹³å‡æµªè´¹ç‡: `BLOCK_SIZE / 2` per request

#### æŠ€æœ¯ 2: Copy-on-Write (å†™æ—¶å¤åˆ¶)

ç”¨äºå®ç°é«˜æ•ˆçš„å†…å­˜å…±äº«ã€‚

**Parallel Sampling ç¤ºä¾‹**ï¼š

```
åˆå§‹çŠ¶æ€: 2 ä¸ª sample å…±äº« prompt
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Shared KV Cache (Prompt)        â”‚
â”‚  Block 0 (Ref=2)  â”‚  Block 1 (Ref=2)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                  â”‚
      Sample A1           Sample A2

ç”Ÿæˆé˜¶æ®µ: Sample A1 éœ€è¦å†™å…¥ Block 1
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Block 0 (Ref=2)  â”‚  Block 1 (Ref=1)   â”‚ <- A2 ç‹¬å 
â”‚                   â”‚  Block 3 (Ref=1)   â”‚ <- A1 çš„å‰¯æœ¬
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                    â”‚
     è§¦å‘ CoW           A1: Block 0 -> Block 3
     Ref--             A2: Block 0 -> Block 1
```

**å®ç°ç»†èŠ‚**ï¼š

```python
def write_to_block(request_id, block_id, kv_data):
    block_entry = block_table[request_id][block_id]
    physical_block = block_entry.physical_block_id

    if block_entry.ref_count > 1:
        # Copy-on-Write
        new_physical = allocator.allocate(1)
        copy_block_data(physical_block, new_physical)

        # æ›´æ–°å¼•ç”¨
        block_entry.ref_count -= 1
        block_entry.physical_block_id = new_physical
        block_entry.ref_count = 1

    # å†™å…¥æ•°æ®
    write_data(block_entry.physical_block_id, kv_data)
```

#### æŠ€æœ¯ 3: Beam Search çš„åŠ¨æ€å…±äº«

Beam Search çš„å…±äº«æ¨¡å¼æ¯” Parallel Sampling æ›´å¤æ‚ã€‚

**æ¼”åŒ–è¿‡ç¨‹**ï¼š

```
åˆå§‹ (4 ä¸ª beam candidates):
         â”Œâ”€ Block 0 (Prompt)
         â”œâ”€ Candidate 0: Block 0 -> Block 1 -> Block 2 -> Block 4
         â”œâ”€ Candidate 1: Block 0 -> Block 1 -> Block 3 -> Block 6
         â”œâ”€ Candidate 2: Block 0 -> Block 1 -> Block 3 -> Block 7
         â””â”€ Candidate 3: Block 0 -> Block 5 -> Block 8 -> Block 9

ç­›é€‰å (ä¿ç•™ top-2ï¼Œéƒ½æ¥è‡ª Candidate 1 å’Œ 2):
         â”Œâ”€ Block 0 (Prompt)
         â”œâ”€ New Candidate 0: Block 0 -> Block 1 -> Block 3 -> Block 6 -> Block 10
         â””â”€ New Candidate 1: Block 0 -> Block 1 -> Block 3 -> Block 7 -> Block 11

é‡Šæ”¾çš„å—: Block 2, 4, 5, 8, 9 (Ref=0)
å…±äº«çš„å—: Block 0 (Ref=2), Block 1 (Ref=2), Block 3 (Ref=2)
```

**å†…å­˜èŠ‚çœ**ï¼š
- ShareGPT æ•°æ®é›†: 44.3% - 66.3%
- Alpaca æ•°æ®é›†: 37.6% - 55.2%

#### æŠ€æœ¯ 4: Shared Prefix

é¢„å…ˆç¼“å­˜å¸¸ç”¨çš„ system promptã€‚

**åº”ç”¨åœºæ™¯**ï¼š

```
æœºå™¨ç¿»è¯‘æœåŠ¡:

System Prompt (341 tokens):
"Translate English to French:
'sea otter' => 'loutre de mer'
'peppermint' => 'menthe poivrÃ©e'
'plush giraffe' => 'girafe en peluche'"

Request A: [System Prompt] + "'cheese' =>"
Request B: [System Prompt] + "'I love you' =>"

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Shared Prefix Blocks (é¢„ç¼“å­˜)     â”‚
â”‚   Block 0, 1, 2, ..., N            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚              â”‚
       Request A      Request B
     åªéœ€å¤„ç†         åªéœ€å¤„ç†
     "'cheese'"    "'I love you'"
```

**å®ç°**ï¼š

```python
# æœåŠ¡å¯åŠ¨æ—¶é¢„è®¡ç®—
def cache_system_prompt(prompt_text):
    tokens = tokenize(prompt_text)
    physical_blocks = allocator.allocate_permanent(
        ceil(len(tokens) / BLOCK_SIZE)
    )

    # è®¡ç®— KV Cache
    kv_cache = model.compute_kv_cache(tokens)
    write_to_blocks(physical_blocks, kv_cache)

    return SharedPrefixHandle(physical_blocks)

# å¤„ç†è¯·æ±‚æ—¶ç›´æ¥å¼•ç”¨
def process_request_with_prefix(prefix_handle, user_input):
    block_table = prefix_handle.blocks.copy()
    block_table[-1].mark_copy_on_write()  # æœ€åä¸€å—å¯èƒ½è¢«ä¿®æ”¹

    # åªå¤„ç†ç”¨æˆ·è¾“å…¥
    process_tokens(user_input, block_table)
```

**æ€§èƒ½æå‡**ï¼š
- 1-shot prefix: 1.67Ã— ååé‡
- 5-shot prefix: 3.58Ã— ååé‡

### 3.3 è°ƒåº¦ä¸æŠ¢å æœºåˆ¶

#### è°ƒåº¦ç­–ç•¥

**FCFS (First-Come-First-Serve)**ï¼š
- ä¿è¯å…¬å¹³æ€§
- é˜²æ­¢è¯·æ±‚é¥¥é¥¿
- æŠ¢å æ—¶ä¼˜å…ˆä¿ç•™æ—©åˆ°çš„è¯·æ±‚

#### æŠ¢å æœºåˆ¶

å½“ GPU å†…å­˜ä¸è¶³æ—¶çš„ä¸¤ç§ç­–ç•¥ï¼š

**ç­–ç•¥ A: Swapping (æ¢å‡ºåˆ° CPU)**

```python
def swap_out_request(request_id):
    blocks = block_table[request_id]

    # åˆ†é… CPU å†…å­˜
    cpu_blocks = cpu_allocator.allocate(len(blocks))

    # ä¼ è¾“æ•°æ®: GPU -> CPU
    for gpu_block, cpu_block in zip(blocks, cpu_blocks):
        copy_device_to_host(gpu_block, cpu_block)

    # é‡Šæ”¾ GPU å†…å­˜
    gpu_allocator.free(blocks)

    # è®°å½•æ˜ å°„
    swapped_table[request_id] = cpu_blocks

def swap_in_request(request_id):
    cpu_blocks = swapped_table[request_id]

    # åˆ†é… GPU å†…å­˜
    gpu_blocks = gpu_allocator.allocate(len(cpu_blocks))

    # ä¼ è¾“æ•°æ®: CPU -> GPU
    for cpu_block, gpu_block in zip(cpu_blocks, gpu_blocks):
        copy_host_to_device(cpu_block, gpu_block)

    # æ¢å¤æ˜ å°„
    block_table[request_id] = gpu_blocks
```

**ä¼˜ç‚¹**ï¼š
- å¯ä»¥æ¢å¤å®Œæ•´çŠ¶æ€
- é€‚åˆå¤§ block size

**ç¼ºç‚¹**ï¼š
- PCIe å¸¦å®½é™åˆ¶
- å¢åŠ å»¶è¿Ÿ

**ç­–ç•¥ B: Recomputation (é‡æ–°è®¡ç®—)**

```python
def evict_request(request_id):
    blocks = block_table[request_id]

    # ä¿å­˜ token åºåˆ—ï¼ˆè½»é‡çº§ï¼‰
    token_sequence = save_tokens(request_id)
    evicted_tokens[request_id] = token_sequence

    # ç›´æ¥é‡Šæ”¾å†…å­˜
    gpu_allocator.free(blocks)
    del block_table[request_id]

def restore_request(request_id):
    tokens = evicted_tokens[request_id]

    # é‡æ–°è®¡ç®— KV Cacheï¼ˆä½œä¸ºä¸€ä¸ªå¤§çš„ prefillï¼‰
    block_table[request_id] = allocate_and_compute(tokens)
```

**ä¼˜ç‚¹**ï¼š
- æ— æ•°æ®ä¼ è¾“å¼€é”€
- é€‚åˆå° block size
- å¯ä»¥åˆ©ç”¨ GPU è®¡ç®—èƒ½åŠ›

**ç¼ºç‚¹**ï¼š
- éœ€è¦é‡æ–°è®¡ç®—
- å¢åŠ è®¡ç®—å¼€é”€

**æ€§èƒ½å¯¹æ¯”ï¼ˆå›¾ 19ï¼‰**ï¼š

| Block Size | Swapping å¼€é”€ | Recomputation å¼€é”€ | æ¨èç­–ç•¥ |
|-----------|--------------|-------------------|---------|
| 1-8       | é«˜           | ä½                | Recomputation |
| 16-64     | ä¸­ç­‰         | ä¸­ç­‰              | ä¸¤è€…ç›¸å½“ |
| 128-256   | ä½           | é«˜                | Swapping |

### 3.4 åˆ†å¸ƒå¼æ‰§è¡Œ

#### Tensor Parallelism

vLLM é‡‡ç”¨ Megatron-LM é£æ ¼çš„å¼ é‡å¹¶è¡Œï¼š

```
å•ä¸ª Attention Layer çš„åˆ†å¸ƒå¼è®¡ç®—:

GPU 0:  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Heads 0-3  â”‚  <- å¤„ç†éƒ¨åˆ† attention heads
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
        All-Reduce
             â†“
GPU 1:  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Heads 4-7  â”‚  <- å¤„ç†å…¶ä»– attention heads
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
        All-Reduce
             â†“
         åˆå¹¶ç»“æœ
```

#### ç»Ÿä¸€çš„ KV Cache ç®¡ç†

å…³é”®è§‚å¯Ÿï¼š
- æ‰€æœ‰ GPU å¤„ç†ç›¸åŒçš„è¾“å…¥ tokens
- å› æ­¤éœ€è¦ç›¸åŒä½ç½®çš„ KV Cache
- æ¯ä¸ª GPU åªå­˜å‚¨è‡ªå·±è´Ÿè´£çš„ attention heads çš„ KV

```python
# è°ƒåº¦å™¨å¹¿æ’­æ¶ˆæ¯
def schedule_step(batch_requests):
    # å‡†å¤‡è¾“å…¥
    input_tokens = [req.next_token for req in batch_requests]

    # å‡†å¤‡ block tablesï¼ˆæ‰€æœ‰ GPU å…±äº«ï¼‰
    block_tables = {
        req.id: block_table[req.id]
        for req in batch_requests
    }

    # å¹¿æ’­ç»™æ‰€æœ‰ workers
    for worker in gpu_workers:
        worker.execute_step(input_tokens, block_tables)

# GPU Worker æ‰§è¡Œ
def execute_step(input_tokens, block_tables):
    # æ ¹æ® block tables è¯»å– KV Cache
    kv_cache = []
    for req_id in input_tokens.keys():
        blocks = block_tables[req_id]
        # åªè¯»å–æœ¬ GPU è´Ÿè´£çš„ heads
        kv_cache.append(
            read_blocks(blocks, head_slice=my_head_range)
        )

    # æ‰§è¡Œæ¨¡å‹è®¡ç®—ï¼ˆå« All-Reduceï¼‰
    outputs = model.forward(input_tokens, kv_cache)

    # è¿”å›ç»“æœç»™è°ƒåº¦å™¨
    return outputs
```

**ä¼˜åŠ¿**ï¼š
- ç®€åŒ–åˆ†å¸ƒå¼é€»è¾‘
- å‡å°‘é€šä¿¡å¼€é”€ï¼ˆåªåœ¨å¿…è¦æ—¶åŒæ­¥ï¼‰
- ä¿æŒå•èŠ‚ç‚¹çš„çµæ´»æ€§

---

## å››ã€å®éªŒç»“æœä¸æ€§èƒ½åˆ†æ

### 4.1 å®éªŒè®¾ç½®

#### æ¨¡å‹é…ç½®

| æ¨¡å‹ | å‚æ•°é‡ | GPU é…ç½® | æ€»æ˜¾å­˜ | KV Cache å¯ç”¨ |
|-----|-------|---------|-------|--------------|
| OPT-13B | 13B | 1Ã— A100 | 40 GB | 12 GB |
| OPT-66B | 66B | 4Ã— A100 | 160 GB | 21 GB |
| OPT-175B | 175B | 8Ã— A100-80GB | 640 GB | 264 GB |

#### æ•°æ®é›†

**ShareGPT** (çœŸå®å¯¹è¯æ•°æ®):
- æ¥æº: ChatGPT ç”¨æˆ·åˆ†äº«çš„å¯¹è¯
- å¹³å‡è¾“å…¥é•¿åº¦: 161 tokens
- å¹³å‡è¾“å‡ºé•¿åº¦: 338 tokens
- ç‰¹ç‚¹: é•¿åºåˆ—ï¼Œé«˜æ–¹å·®

**Alpaca** (æŒ‡ä»¤æ•°æ®):
- æ¥æº: GPT-3.5 ç”Ÿæˆçš„æŒ‡ä»¤æ•°æ®
- å¹³å‡è¾“å…¥é•¿åº¦: 19 tokens
- å¹³å‡è¾“å‡ºé•¿åº¦: 58 tokens
- ç‰¹ç‚¹: çŸ­åºåˆ—ï¼Œä½æ–¹å·®

#### Baseline ç³»ç»Ÿ

1. **FasterTransformer**
   - NVIDIA å®˜æ–¹æ¨ç†å¼•æ“
   - ä¼˜åŒ–ç›®æ ‡ï¼šä½å»¶è¿Ÿ
   - åŠ¨æ€ batchingï¼Œå›ºå®š batch size ä¸Šé™

2. **Orca (Max)**
   - é¢„ç•™æœ€å¤§é•¿åº¦ (2048 tokens)
   - æœ€ä¿å®ˆï¼Œæµªè´¹æœ€å¤š

3. **Orca (Pow2)**
   - é¢„ç•™åˆ° 2 çš„å¹‚æ¬¡ï¼ˆæœ€å¤š 2Ã— å®é™…é•¿åº¦ï¼‰
   - ä¾‹: å®é™… 25 tokensï¼Œé¢„ç•™ 32

4. **Orca (Oracle)**
   - å‡è®¾çŸ¥é“çœŸå®è¾“å‡ºé•¿åº¦
   - ç†è®ºä¸Šç•Œï¼Œå®é™…ä¸å¯è¡Œ

#### è¯„ä¼°æŒ‡æ ‡

**Normalized Latency** (å½’ä¸€åŒ–å»¶è¿Ÿ):
```
normalized_latency = mean(request.latency / request.output_length)
```

å•ä½: ç§’/tokenï¼Œè¶Šä½è¶Šå¥½

**Throughput** (ååé‡):
- ç³»ç»Ÿèƒ½ç»´æŒä½å»¶è¿Ÿçš„æœ€å¤§è¯·æ±‚é€Ÿç‡
- å•ä½: requests/second

### 4.2 å•åºåˆ—ç”Ÿæˆæ€§èƒ½

#### ShareGPT æ•°æ®é›†ï¼ˆé•¿åºåˆ—ï¼‰

| æ¨¡å‹ | è¯·æ±‚ç‡ (req/s) | vLLM vs Orca (Oracle) | vLLM vs FasterTransformer |
|-----|---------------|----------------------|--------------------------|
| OPT-13B | 2.0 | **2.2Ã—** | **22Ã—** |
| OPT-66B | 1.0 | **2.7Ã—** | **11Ã—** |
| OPT-175B | 2.5 | **1.7Ã—** | **8Ã—** |

#### Alpaca æ•°æ®é›†ï¼ˆçŸ­åºåˆ—ï¼‰

| æ¨¡å‹ | è¯·æ±‚ç‡ (req/s) | vLLM vs Orca (Oracle) | vLLM vs FasterTransformer |
|-----|---------------|----------------------|--------------------------|
| OPT-13B | 30 | **2.5Ã—** | **15Ã—** |
| OPT-66B | 20 | **2.1Ã—** | **12Ã—** |
| OPT-175B | 20 | **1.3Ã—** | **6Ã—** |

> ğŸ“Š **æ³¨**: OPT-175B åœ¨ Alpaca ä¸Šæå‡è¾ƒå°ï¼Œå› ä¸ºæ­¤é…ç½®ä¸‹æˆä¸ºè®¡ç®—ç“¶é¢ˆè€Œéå†…å­˜ç“¶é¢ˆ

#### Batch Size åˆ†æ

**OPT-13B @ ShareGPT (2 req/s)**:

```
å¹³å‡å¹¶å‘è¯·æ±‚æ•°:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ç³»ç»Ÿ          â”‚ Batch Size    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Orca (Max)   â”‚  7.00         â”‚
â”‚ Orca (Pow2)  â”‚  9.81  (â†‘40%) â”‚
â”‚ Orca (Oracle)â”‚ 13.62  (â†‘95%) â”‚
â”‚ vLLM         â”‚ 30.42  (â†‘335%)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**OPT-13B @ Alpaca (30 req/s)**:

```
å¹³å‡å¹¶å‘è¯·æ±‚æ•°:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ç³»ç»Ÿ          â”‚ Batch Size    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Orca (Max)   â”‚   7.00        â”‚
â”‚ Orca (Pow2)  â”‚  43.24 (â†‘518%)â”‚
â”‚ Orca (Oracle)â”‚  72.75 (â†‘939%)â”‚
â”‚ vLLM         â”‚ 132.44 (â†‘1792%)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> ğŸ’¡ **å…³é”®æ´å¯Ÿ**: vLLM èƒ½å¤Ÿæ‰¹å¤„ç† **4-19Ã— æ›´å¤šè¯·æ±‚**ï¼Œå……åˆ†åˆ©ç”¨ GPU è®¡ç®—èƒ½åŠ›

### 4.3 å¤æ‚è§£ç ç®—æ³•æ€§èƒ½

#### Parallel Sampling

æµ‹è¯•é…ç½®: OPT-13B, Alpaca æ•°æ®é›†

| å¹¶è¡Œæ•° | vLLM vs Orca (Oracle) | å†…å­˜èŠ‚çœ |
|-------|----------------------|---------|
| 2     | 1.5Ã— ååé‡           | 6.09%   |
| 4     | 1.8Ã— ååé‡           | 8.53%   |
| 6     | 2.1Ã— ååé‡           | 9.79%   |

#### Beam Search

æµ‹è¯•é…ç½®: OPT-13B, Alpaca æ•°æ®é›†

| Beam Width | vLLM vs Orca (Oracle) | å†…å­˜èŠ‚çœ |
|-----------|----------------------|---------|
| 2         | 1.6Ã— ååé‡           | 37.56%  |
| 4         | 2.0Ã— ååé‡           | 53.13%  |
| 6         | **2.3Ã— ååé‡**       | **55.16%** |

> ğŸ“ˆ **è¶‹åŠ¿**: Beam width è¶Šå¤§ï¼ŒvLLM çš„ä¼˜åŠ¿è¶Šæ˜æ˜¾

#### ShareGPT æ•°æ®é›†å¯¹æ¯”

åœ¨æ›´é•¿çš„åºåˆ—ä¸Šï¼Œå†…å­˜å…±äº«æ•ˆæœæ›´æ˜¾è‘—ï¼š

| è§£ç æ–¹æ³• | å†…å­˜èŠ‚çœ (Alpaca) | å†…å­˜èŠ‚çœ (ShareGPT) |
|---------|-----------------|-------------------|
| Parallel Sampling (n=6) | 9.79% | 30.5% |
| Beam Search (width=6) | 55.16% | **66.3%** |

### 4.4 Shared Prefix æ€§èƒ½

æµ‹è¯•é…ç½®: LLaMA-13B, WMT16 ç¿»è¯‘ä»»åŠ¡

**1-shot prefix (80 tokens)**:
- vLLM ååé‡: 42 req/s
- Orca (Oracle) ååé‡: 25 req/s
- **æå‡: 1.67Ã—**

**5-shot prefix (341 tokens)**:
- vLLM ååé‡: 43 req/s
- Orca (Oracle) ååé‡: 12 req/s
- **æå‡: 3.58Ã—**

> ğŸ’¡ **å‘ç°**: Prefix è¶Šé•¿ï¼ŒvLLM çš„ä¼˜åŠ¿è¶Šæ˜¾è‘—

### 4.5 èŠå¤©æœºå™¨äººåœºæ™¯

æµ‹è¯•é…ç½®: OPT-13B, ShareGPT å¯¹è¯æ•°æ®

ç‰¹ç‚¹:
- é•¿ä¸Šä¸‹æ–‡ (æœ€å¤š 1024 tokens)
- è¿ç»­å¤šè½®å¯¹è¯
- Prompt å æ¯”å¤§

**ç»“æœ**:

```
æœ€å¤§å¯æŒç»­è¯·æ±‚ç‡:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Orca (Max)   â”‚ 0.4 req/s     â”‚
â”‚ Orca (Pow2)  â”‚ 0.4 req/s     â”‚
â”‚ Orca (Oracle)â”‚ 0.4 req/s     â”‚
â”‚ vLLM         â”‚ 0.8 req/s     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æå‡: 2Ã— ååé‡
```

åŸå› : é•¿ prompt å¯¼è‡´ Orca çš„ Buddy allocator æµªè´¹ä¸¥é‡

---

## äº”ã€å…³é”®å®ç°ç»†èŠ‚

### 5.1 Block Size é€‰æ‹©

#### æ¶ˆèå®éªŒ

æµ‹è¯•é…ç½®: OPT-13B, å›ºå®šè¯·æ±‚ç‡

**ShareGPT æ•°æ®é›†**:

```
Block Size vs Normalized Latency:
   1 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 12.1 s/token
   2 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     8.5  s/token
   4 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      7.2  s/token
   8 â–ˆâ–ˆâ–ˆâ–ˆ         4.8  s/token
  16 â–ˆâ–ˆ           2.1  s/token  â† æœ€ä¼˜
  32 â–ˆâ–ˆ           2.3  s/token
  64 â–ˆâ–ˆâ–ˆ          3.1  s/token
 128 â–ˆâ–ˆâ–ˆâ–ˆ         4.2  s/token
 256 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     8.7  s/token
```

**Alpaca æ•°æ®é›†**:

```
Block Size vs Normalized Latency:
   1 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       6.2 s/token
   2 â–ˆâ–ˆâ–ˆâ–ˆ         4.1 s/token
   4 â–ˆâ–ˆâ–ˆ          3.2 s/token
   8 â–ˆâ–ˆ           2.5 s/token
  16 â–ˆ            1.8 s/token  â† æœ€ä¼˜
  32 â–ˆ            1.9 s/token  â† æ¬¡ä¼˜
  64 â–ˆâ–ˆâ–ˆâ–ˆ         4.2 s/token
 128 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  10.5 s/token  (åºåˆ—å¤ªçŸ­)
 256 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ15.2 s/token  (ä¸¥é‡æµªè´¹)
```

#### è®¾è®¡æƒè¡¡

**Block size å¤ªå° (1-8)**:
- âŒ æ— æ³•å……åˆ†åˆ©ç”¨ GPU å¹¶è¡Œæ€§
- âŒ Block table æŸ¥æ‰¾å¼€é”€å¤§
- âŒ Kernel launch æ¬¡æ•°å¤š

**Block size å¤ªå¤§ (128-256)**:
- âŒ å†…éƒ¨ç¢ç‰‡ä¸¥é‡
- âŒ å…±äº«ç²’åº¦ç²—
- âŒ å¯¹çŸ­åºåˆ—ç‰¹åˆ«ä¸å‹å¥½

**æœ€ä¼˜é€‰æ‹©: 16**:
- âœ… å¹³è¡¡å¹¶è¡Œæ€§å’Œç¢ç‰‡ç‡
- âœ… é€‚åˆå¤§å¤šæ•°åºåˆ—é•¿åº¦
- âœ… vLLM çš„é»˜è®¤é…ç½®

### 5.2 Kernel ä¼˜åŒ–

#### ä¸‰å¤§èåˆå†…æ ¸

**1. Fused Reshape + Block Write**

ä¼ ç»Ÿæ–¹æ³•:
```cuda
// 3 æ¬¡ kernel launch
reshape_kernel<<<...>>>(kv_cache);
transpose_kernel<<<...>>>(kv_cache);
write_kernel<<<...>>>(kv_cache, block_table);
```

vLLM ä¼˜åŒ–:
```cuda
// 1 æ¬¡ kernel launch
fused_reshape_write_kernel<<<...>>>(
    kv_cache, block_table, block_size
);
```

**2. Fused Block Read + Attention**

ä¼ ç»Ÿæ–¹æ³•:
```cuda
// åˆ†ç¦»çš„è¯»å–å’Œè®¡ç®—
for (int block_id : block_table) {
    read_block_kernel<<<...>>>(block_id, kv_buffer);
    __syncthreads();
    attention_kernel<<<...>>>(query, kv_buffer, output);
}
```

vLLM ä¼˜åŒ–:
```cuda
// èåˆè¯»å–å’Œè®¡ç®—
paged_attention_kernel<<<...>>>(
    query, block_table, block_size, output
) {
    // åœ¨ kernel å†…éƒ¨:
    // 1. æŒ‰éœ€è¯»å–å—
    // 2. è®¡ç®— attention
    // 3. ç´¯åŠ ç»“æœ
}
```

å…³é”®æŠ€æœ¯:
- **æ¯ä¸ª warp å¤„ç†ä¸€ä¸ªå—**ï¼Œä¿è¯ coalesced memory access
- **æ”¯æŒå˜é•¿åºåˆ—**ï¼Œæ— éœ€ padding
- **On-the-fly è®¡ç®—**ï¼Œå‡å°‘ä¸­é—´ç»“æœå­˜å‚¨

**3. Fused Block Copy**

ç”¨äº Copy-on-Write:

ä¼ ç»Ÿæ–¹æ³•:
```cuda
// å¤šæ¬¡å°æ•°æ®ä¼ è¾“
for (int block_id : blocks_to_copy) {
    cudaMemcpyAsync(dst[block_id], src[block_id],
                     block_size, stream);
}
// é—®é¢˜: launch overhead é«˜
```

vLLM ä¼˜åŒ–:
```cuda
// æ‰¹é‡å¤åˆ¶ kernel
batched_block_copy_kernel<<<...>>>(
    src_blocks, dst_blocks, num_blocks, block_size
);
```

#### æ€§èƒ½å¼€é”€åˆ†æ

**Attention Kernel å¾®åŸºå‡†æµ‹è¯•** (å›¾ 18a):

| Batch Size | Context Length | vLLM å»¶è¿Ÿ | FasterTransformer å»¶è¿Ÿ | å¼€é”€ |
|-----------|---------------|----------|----------------------|------|
| 8         | 64            | 45 Î¼s    | 36 Î¼s                | +25% |
| 8         | 128           | 78 Î¼s    | 64 Î¼s                | +22% |
| 8         | 256           | 142 Î¼s   | 112 Î¼s               | +27% |
| 32        | 64            | 52 Î¼s    | 42 Î¼s                | +24% |
| 32        | 128           | 91 Î¼s    | 73 Î¼s                | +25% |
| 32        | 256           | 168 Î¼s   | 135 Î¼s               | +24% |

**å¼€é”€æ¥æº**:
1. Block table æŸ¥æ‰¾ (~5%)
2. éè¿ç»­å†…å­˜è®¿é—® (~10%)
3. åˆ†æ”¯å¼€é”€ (~5%)
4. å˜é•¿åºåˆ—å¤„ç† (~5%)

**ä½†æ˜¯**:
- Attention åªå æ¨¡å‹æ€»è®¡ç®—çš„ ~15%
- ç«¯åˆ°ç«¯å¼€é”€: < 4%
- æ¢æ¥ 2-4Ã— çš„ååé‡æå‡

### 5.3 Swapping vs Recomputation

#### å¾®åŸºå‡†æµ‹è¯•

æµ‹è¯•é…ç½®: OPT-13B, 1024 tokens

**Block Size å¯¹å¼€é”€çš„å½±å“**:

| Block Size | Swap Out | Swap In | Swap (æ€») | Recompute |
|-----------|---------|---------|----------|-----------|
| 1         | 85 ms   | 82 ms   | 167 ms   | 45 ms âœ“   |
| 16        | 18 ms   | 16 ms   | 34 ms    | 45 ms     |
| 64        | 9 ms    | 8 ms    | 17 ms âœ“  | 45 ms     |
| 256       | 7 ms    | 6 ms    | 13 ms âœ“  | 45 ms     |

> ğŸ’¡ **å…³é”®æ´å¯Ÿ**:
> - Recomputation å¼€é”€ä¸ block size æ— å…³
> - Swapping éš block size å¢å¤§è€Œå‡å°‘ï¼ˆPCIe å¸¦å®½åˆ©ç”¨æ›´å¥½ï¼‰

#### ç«¯åˆ°ç«¯æ€§èƒ½

æµ‹è¯•é…ç½®: OPT-13B, ShareGPT, åŒç­‰è¯·æ±‚ç‡

**Normalized Latency**:

| Block Size | Recomputation | Swapping |
|-----------|--------------|----------|
| 1         | 1.2 s/token  | 2.1 s/token |
| 16        | 1.8 s/token  | 1.7 s/token |
| 64        | 2.0 s/token  | 1.5 s/token |
| 256       | 2.2 s/token  | 1.4 s/token |

#### æ¨èç­–ç•¥

```python
def choose_eviction_policy(block_size, gpu_compute_power, pcie_bandwidth):
    if block_size <= 16:
        return "recomputation"  # å°å—æ—¶é¿å…ä¼ è¾“å¼€é”€
    elif block_size >= 128:
        return "swapping"       # å¤§å—æ—¶ä¼ è¾“æ•ˆç‡é«˜
    else:
        # æ ¹æ®ç¡¬ä»¶ç‰¹æ€§åŠ¨æ€é€‰æ‹©
        compute_cost = block_size * gpu_compute_power
        transfer_cost = block_size * pcie_bandwidth
        return "recomputation" if compute_cost < transfer_cost else "swapping"
```

### 5.4 æ”¯æŒå¤šç§è§£ç ç®—æ³•

#### æ ¸å¿ƒæ¥å£è®¾è®¡

vLLM æä¾›ä¸‰ä¸ªåŸºç¡€æ“ä½œ:

```python
class SequenceManager:
    def fork(self, parent_id: int) -> int:
        """åˆ›å»ºæ–°åºåˆ—ï¼Œå…±äº«çˆ¶åºåˆ—çš„ KV Cache"""
        child_id = allocate_sequence_id()

        # å¤åˆ¶ block tableï¼ˆæµ…æ‹·è´ï¼‰
        block_table[child_id] = block_table[parent_id].copy()

        # å¢åŠ å¼•ç”¨è®¡æ•°
        for block in block_table[child_id]:
            block.ref_count += 1

        return child_id

    def append(self, seq_id: int, token: int, kv: Tensor):
        """æ·»åŠ æ–° token çš„ KV Cache"""
        last_block = block_table[seq_id][-1]

        if last_block.is_full():
            # åˆ†é…æ–°å—
            new_block = allocator.allocate(1)
            block_table[seq_id].append(new_block)
            last_block = new_block

        # å†™å…¥ï¼ˆè§¦å‘ CoWï¼‰
        if last_block.ref_count > 1:
            last_block = copy_on_write(last_block)

        write_to_block(last_block, kv)

    def free(self, seq_id: int):
        """åˆ é™¤åºåˆ—ï¼Œé‡Šæ”¾ KV Cache"""
        for block in block_table[seq_id]:
            block.ref_count -= 1
            if block.ref_count == 0:
                allocator.free(block)

        del block_table[seq_id]
```

#### è§£ç ç®—æ³•å®ç°

**Parallel Sampling**:

```python
def parallel_sampling(prompt, num_samples):
    # 1. Prefill
    base_seq_id = process_prompt(prompt)

    # 2. Fork multiple samples
    sample_ids = [seq_manager.fork(base_seq_id) for _ in range(num_samples)]

    # 3. Independent generation
    while not all_finished(sample_ids):
        for seq_id in sample_ids:
            if not is_finished(seq_id):
                token, kv = generate_token(seq_id)
                seq_manager.append(seq_id, token, kv)

    # 4. Cleanup
    seq_manager.free(base_seq_id)
    return [get_output(sid) for sid in sample_ids]
```

**Beam Search**:

```python
def beam_search(prompt, beam_width, max_len):
    # 1. Prefill
    base_seq_id = process_prompt(prompt)

    # 2. Initialize beams
    beams = [seq_manager.fork(base_seq_id) for _ in range(beam_width)]
    scores = [0.0] * beam_width

    # 3. Beam expansion
    for step in range(max_len):
        candidates = []

        # Expand each beam
        for beam_id, score in zip(beams, scores):
            top_k_tokens, probs = get_top_k(beam_id, vocab_size)

            for token, prob in zip(top_k_tokens, probs):
                child_id = seq_manager.fork(beam_id)
                kv = compute_kv(child_id, token)
                seq_manager.append(child_id, token, kv)

                candidates.append((child_id, score + log(prob)))

        # Select top-k
        candidates.sort(key=lambda x: x[1], reverse=True)
        new_beams = [cand[0] for cand in candidates[:beam_width]]
        new_scores = [cand[1] for cand in candidates[:beam_width]]

        # Free old beams
        for old_beam in beams:
            if old_beam not in new_beams:
                seq_manager.free(old_beam)

        beams = new_beams
        scores = new_scores

    # 4. Return best
    best_idx = argmax(scores)
    return get_output(beams[best_idx])
```

---

## å…­ã€æ ¸å¿ƒè´¡çŒ®ä¸å½±å“

### 6.1 ç†è®ºè´¡çŒ®

#### 1. ç³»ç»Ÿæ€§åˆ†æ LLM æ¨ç†çš„å†…å­˜ç“¶é¢ˆ

è®ºæ–‡é¦–æ¬¡é‡åŒ–äº† KV Cache å†…å­˜ç®¡ç†çš„é‡è¦æ€§ï¼š

**å†…å­˜æµªè´¹åˆ†è§£**:
```
æ€»å†…å­˜æµªè´¹ = é¢„ç•™å†…å­˜ + å†…éƒ¨ç¢ç‰‡ + å¤–éƒ¨ç¢ç‰‡
          = 13.3-26.8% + 13.6-17.9% + 25.2-41.6%
          = 60-80%
```

**å¯¹ååé‡çš„å½±å“**:
- å†…å­˜åˆ©ç”¨ç‡æ¯æå‡ 10%
- Batch size å¢åŠ  15-25%
- ååé‡æå‡ 20-30%

#### 2. è·¨é¢†åŸŸæŠ€æœ¯è¿ç§»çš„èŒƒä¾‹

è®ºæ–‡è¯æ˜äº† OS æŠ€æœ¯å¯ä»¥æœ‰æ•ˆåº”ç”¨äº AI ç³»ç»Ÿï¼š

| OS æŠ€æœ¯ | AI ç³»ç»Ÿåº”ç”¨ | æ•ˆæœ |
|--------|-----------|------|
| è™šæ‹Ÿå†…å­˜ | KV Cache é€»è¾‘è§†å›¾ | ç®€åŒ–ç¼–ç¨‹æ¨¡å‹ |
| åˆ†é¡µ | Block-level ç®¡ç† | æ¶ˆé™¤ç¢ç‰‡ |
| Page Table | Block Table | çµæ´»æ˜ å°„ |
| Copy-on-Write | KV Cache å…±äº« | èŠ‚çœ 30-66% å†…å­˜ |
| Swapping | GPU â†” CPU | æ”¯æŒè¶…å¤§ batch |

#### 3. æå‡ºåˆ†å— Attention çš„ç†è®ºæ¡†æ¶

**æ•°å­¦ç­‰ä»·æ€§è¯æ˜**:

ä¼ ç»Ÿ Attention:
```
o_i = Î£_{j=1}^i softmax(q_i^T k_j / âˆšd) Â· v_j
```

PagedAttention (å—çº§):
```
o_i = Î£_{b=1}^âŒˆi/BâŒ‰ softmax(q_i^T K_b / âˆšd) Â· V_b
```

è¯æ˜: ä¸¤è€…åœ¨æ•°å€¼ä¸Šå®Œå…¨ç­‰ä»·ï¼ˆå¿½ç•¥æµ®ç‚¹è¯¯å·®ï¼‰

**é€šç”¨æ€§**:
- é€‚ç”¨äºæ‰€æœ‰ Transformer æ¶æ„
- æ”¯æŒå„ç§ attention å˜ä½“ï¼ˆMulti-head, Multi-query, Grouped-queryï¼‰
- å¯æ‰©å±•åˆ° Cross-attention

### 6.2 å·¥ç¨‹è´¡çŒ®

#### 1. è¿‘é›¶å†…å­˜æµªè´¹

**å¯¹æ¯”åˆ†æ**:

| ç³»ç»Ÿ | æœ‰æ•ˆåˆ©ç”¨ç‡ | æµªè´¹ç‡ | æ”¹è¿› |
|-----|----------|--------|------|
| FasterTransformer | ~20% | ~80% | - |
| Orca (Max) | 20.4% | 79.6% | - |
| Orca (Pow2) | 38.2% | 61.8% | 1.9Ã— |
| Orca (Oracle) | 57.3% | 42.7% | 2.8Ã— |
| **vLLM** | **96.3%** | **3.7%** | **4.7Ã—** |

#### 2. æ˜¾è‘—çš„ååé‡æå‡

**ä¸åŒåœºæ™¯çš„æå‡**:

| åœºæ™¯ | æ¨¡å‹ | æ•°æ®é›† | vLLM vs Orca | vLLM vs FasterTransformer |
|-----|------|-------|-------------|--------------------------|
| å•åºåˆ— | OPT-13B | ShareGPT | 2.2Ã— | 22Ã— |
| å•åºåˆ— | OPT-175B | Alpaca | 1.3Ã— | 6Ã— |
| Parallel (n=6) | OPT-13B | Alpaca | 2.1Ã— | - |
| Beam (w=6) | OPT-13B | Alpaca | 2.3Ã— | - |
| Shared Prefix | LLaMA-13B | WMT16 | 3.6Ã— | - |
| Chatbot | OPT-13B | ShareGPT | 2.0Ã— | - |

#### 3. ç»Ÿä¸€çš„ç¼–ç¨‹æ¥å£

vLLM æä¾›äº†ç®€æ´çš„ APIï¼Œéšè—äº†å¤æ‚çš„å†…å­˜ç®¡ç†ï¼š

```python
from vllm import LLM, SamplingParams

# åˆ›å»ºæ¨¡å‹
llm = LLM(model="meta-llama/Llama-2-7b-hf")

# å„ç§è§£ç ç®—æ³•ä½¿ç”¨ç›¸åŒçš„æ¥å£

# 1. åŸºç¡€é‡‡æ ·
outputs = llm.generate(prompts, SamplingParams(
    temperature=0.8,
    top_p=0.95,
))

# 2. Beam search
outputs = llm.generate(prompts, SamplingParams(
    use_beam_search=True,
    best_of=5,
))

# 3. Parallel sampling
outputs = llm.generate(prompts, SamplingParams(
    n=5,  # ç”Ÿæˆ 5 ä¸ªå€™é€‰
    temperature=1.0,
))

# åº•å±‚è‡ªåŠ¨å¤„ç†å†…å­˜å…±äº«å’Œç®¡ç†
```

### 6.3 å®é™…å½±å“

#### å¼€æºç¤¾åŒº

**GitHub ç»Ÿè®¡** (æˆªè‡³è®ºæ–‡å‘è¡¨):
- â­ Stars: 20,000+
- ğŸ”± Forks: 2,000+
- ğŸ’¬ Issues: 1,500+
- ğŸ‘¥ Contributors: 100+

#### å·¥ä¸šé‡‡ç”¨

**ä½¿ç”¨ vLLM çš„å…¬å¸/é¡¹ç›®**:
- Anyscale (Ray ç”Ÿæ€)
- LMSys (Chatbot Arena)
- HuggingFace (Text Generation Inference)
- OpenAI (å†…éƒ¨æµ‹è¯•)
- Anthropic (å†…éƒ¨æµ‹è¯•)

#### åç»­å·¥ä½œå¯å‘

**ç›´æ¥å½±å“çš„ç³»ç»Ÿ**:
1. **TensorRT-LLM** (NVIDIA)
   - å€Ÿé‰´ PagedAttention çš„æ€æƒ³
   - å®ç°äº†ç±»ä¼¼çš„åˆ†å—ç®¡ç†

2. **Text Generation Inference** (HuggingFace)
   - é›†æˆ vLLM ä½œä¸ºåç«¯
   - æ”¯æŒæ›´å¤šæ¨¡å‹æ¶æ„

3. **LMDeploy** (å•†æ±¤)
   - å‚è€ƒ vLLM çš„è°ƒåº¦ç­–ç•¥
   - ä¼˜åŒ–å›½äº§ GPU é€‚é…

**å­¦æœ¯å½±å“**:
- SOSP 2023 Best Paper Nominee
- è¢« 50+ è®ºæ–‡å¼•ç”¨
- æˆä¸º LLM æ¨ç†ç³»ç»Ÿçš„æ ‡å‡† baseline

---

## ä¸ƒã€å±€é™ä¸æœªæ¥æ–¹å‘

### 7.1 å½“å‰å±€é™

#### 1. é€‚ç”¨åœºæ™¯é™åˆ¶

**é€‚ç”¨åœºæ™¯** âœ…:
- åœ¨çº¿æœåŠ¡ (åŠ¨æ€è¯·æ±‚åˆ°è¾¾)
- å†…å­˜å—é™ç¯å¢ƒ
- é•¿åºåˆ—ç”Ÿæˆ
- éœ€è¦é«˜ååçš„åº”ç”¨

**ä¸é€‚ç”¨åœºæ™¯** âŒ:
- ç¦»çº¿æ‰¹å¤„ç† (å·²çŸ¥æ‰€æœ‰è¾“å…¥)
- è®¡ç®—ç“¶é¢ˆåœºæ™¯ (å¦‚ INT4 é‡åŒ–æ¨ç†)
- éå¸¸çŸ­çš„åºåˆ— (< 32 tokens)
- é Transformer æ¶æ„

#### 2. Kernel å¼€é”€

**Attention Kernel**:
- æ¯” FasterTransformer æ…¢ 20-26%
- åŸå› : Block table æŸ¥æ‰¾ã€éè¿ç»­è®¿å­˜

**ç«¯åˆ°ç«¯å½±å“**:
- Attention å æ€»æ—¶é—´ ~15%
- å®é™…å¼€é”€ < 4%
- è¢«ååé‡æå‡æŠµæ¶ˆ

#### 3. æœ‰é™çš„è·¨è¯·æ±‚å…±äº«

**å½“å‰å®ç°**:
- âœ… è¯·æ±‚å†…å…±äº« (Parallel Sampling, Beam Search)
- âœ… ç›¸åŒ Prefix å…±äº« (éœ€æ‰‹åŠ¨é…ç½®)
- âŒ è‡ªåŠ¨æ£€æµ‹ç›¸ä¼¼ Prefix
- âŒ éƒ¨åˆ† Prefix åŒ¹é…

**æ½œåœ¨æ”¹è¿›ç©ºé—´**:
- Automatic Prefix Caching
- Semantic-based Sharing

#### 4. CPU-GPU é€šä¿¡å¼€é”€

**Swapping æ€§èƒ½**:
- PCIe 3.0: ~16 GB/s
- PCIe 4.0: ~32 GB/s
- é™åˆ¶äº† swap é€Ÿåº¦

**å¯èƒ½æ”¹è¿›**:
- NVLink æ”¯æŒ
- GPU Direct RDMA
- æ›´æ™ºèƒ½çš„ swap ç­–ç•¥

### 7.2 æœªæ¥ç ”ç©¶æ–¹å‘

#### æ–¹å‘ 1: æ›´ç»†ç²’åº¦çš„å†…å­˜ç®¡ç†

**Token-level Paging**:
```python
# å½“å‰: Block size = 16 tokens
Block = [token_1, ..., token_16]  # å›ºå®šå¤§å°

# æœªæ¥: Variable-size blocks
Block = [token_1, ..., token_k]   # k âˆˆ [1, 32]
```

**ä¼˜åŠ¿**:
- è¿›ä¸€æ­¥å‡å°‘å†…éƒ¨ç¢ç‰‡
- æ›´çµæ´»çš„å…±äº«ç²’åº¦

**æŒ‘æˆ˜**:
- Block table å¼€é”€å¢åŠ 
- Kernel å®ç°å¤æ‚åº¦

#### æ–¹å‘ 2: è·¨è¯·æ±‚çš„æ™ºèƒ½ç¼“å­˜

**Automatic Prefix Caching**:

```python
class PrefixCache:
    def __init__(self):
        self.cache = {}  # prefix -> KV blocks
        self.lru = LRUCache(capacity=1000)

    def get_or_compute(self, prompt):
        # æŸ¥æ‰¾æœ€é•¿å…¬å…±å‰ç¼€
        prefix = self.find_longest_prefix(prompt)

        if prefix in self.cache:
            # å‘½ä¸­ç¼“å­˜
            blocks = self.cache[prefix]
            remaining = prompt[len(prefix):]
            return blocks, remaining
        else:
            # æœªå‘½ä¸­ï¼Œè®¡ç®—å¹¶ç¼“å­˜
            blocks = compute_kv(prompt)
            self.cache[prompt] = blocks
            return blocks, []
```

**åº”ç”¨åœºæ™¯**:
- å¤šè½®å¯¹è¯ï¼ˆå…±äº«å†å²ï¼‰
- Few-shot learningï¼ˆå…±äº«ç¤ºä¾‹ï¼‰
- RAGï¼ˆå…±äº«æ£€ç´¢ç»“æœï¼‰

#### æ–¹å‘ 3: ä¸å…¶ä»–ä¼˜åŒ–æŠ€æœ¯ç»“åˆ

**é‡åŒ– (Quantization)**:

```
FP16 KV Cache:     800 KB / token
INT8 KV Cache:     400 KB / token  (2Ã— å†…å­˜èŠ‚çœ)
INT4 KV Cache:     200 KB / token  (4Ã— å†…å­˜èŠ‚çœ)

vLLM + INT4:       8Ã— å†…å­˜èŠ‚çœ + 96% åˆ©ç”¨ç‡
                   = æ‰¹é‡å¤„ç† 32Ã— æ›´å¤šè¯·æ±‚
```

**ç¨€ç–åŒ– (Sparsity)**:

```python
# H2O: Heavy Hitters Oracle
# ä¿ç•™é‡è¦çš„ KV Cacheï¼Œä¸¢å¼ƒä¸é‡è¦çš„

def compress_kv_cache(kv_cache, keep_ratio=0.5):
    # è®¡ç®—é‡è¦æ€§åˆ†æ•°
    importance = compute_attention_scores(kv_cache)

    # ä¿ç•™ top-k
    k = int(len(kv_cache) * keep_ratio)
    important_indices = topk(importance, k)

    return kv_cache[important_indices]
```

**ç»“åˆæ•ˆæœ**:
- vLLM (å†…å­˜ç®¡ç†) + é‡åŒ– (å®¹é‡) + ç¨€ç– (å†—ä½™)
- ç†è®ºä¸Šå¯æ”¯æŒ **100Ã—+ ååé‡æå‡**

#### æ–¹å‘ 4: å¤šæ¨¡æ€æ¨¡å‹æ‰©å±•

**æŒ‘æˆ˜**:
- å›¾åƒ tokens æ•°é‡å·¨å¤§ (576 for ViT-L)
- ä¸åŒæ¨¡æ€çš„ cache å¤§å°ä¸åŒ
- Cross-attention çš„å†…å­˜ç®¡ç†

**å¯èƒ½æ–¹æ¡ˆ**:

```python
class MultimodalBlockTable:
    text_blocks: List[Block]      # æ–‡æœ¬ KV Cache
    vision_blocks: List[Block]    # è§†è§‰ KV Cache
    cross_blocks: List[Block]     # Cross-attention Cache

    def share_vision_blocks(self, requests):
        # å›¾åƒå¯ä»¥åœ¨å¤šä¸ªè¯·æ±‚é—´å…±äº«
        if same_image(requests):
            for req in requests:
                req.vision_blocks = shared_blocks
```

#### æ–¹å‘ 5: æ¨æµ‹è§£ç  (Speculative Decoding)

**ç»“åˆ vLLM**:

```python
def speculative_decoding_with_vllm(prompt):
    # 1. ç”¨å°æ¨¡å‹å¿«é€Ÿç”Ÿæˆå€™é€‰
    draft_model = vLLM(model="small-llm")
    candidates = draft_model.generate(prompt, n=5)

    # 2. ç”¨å¤§æ¨¡å‹éªŒè¯
    target_model = vLLM(model="large-llm")
    # å…±äº« prompt çš„ KV Cache
    verified = target_model.verify(candidates, share_prefix=prompt)

    return verified
```

**ä¼˜åŠ¿**:
- å‡å°‘å¤§æ¨¡å‹çš„ decode æ­¥æ•°
- vLLM é«˜æ•ˆç®¡ç†ä¸¤ä¸ªæ¨¡å‹çš„ KV Cache

#### æ–¹å‘ 6: ç¡¬ä»¶ååŒè®¾è®¡

**ä¸“ç”¨ç¡¬ä»¶æ”¯æŒ**:

1. **Sparse Block Indexing**
   - ç¡¬ä»¶åŠ é€Ÿ Block Table æŸ¥æ‰¾
   - ç±»ä¼¼ TLB (Translation Lookaside Buffer)

2. **KV Cache Compression**
   - ç¡¬ä»¶æ”¯æŒçš„æ— æŸ/æœ‰æŸå‹ç¼©
   - å‡å°‘å¸¦å®½éœ€æ±‚

3. **CXL Memory**
   - æ‰©å±• GPU å¯è®¿é—®å†…å­˜
   - æ›´å¤§çš„ KV Cache å®¹é‡

**ç¤ºä¾‹æ¶æ„**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GPU Compute Units                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Block Table Cache (Hardware TLB)        â”‚  â† æ–°å¢
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  HBM (High Bandwidth Memory)             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  CXL Memory Expansion                    â”‚  â† æ–°å¢
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## å…«ã€æ€»ç»“

### 8.1 æ ¸å¿ƒåˆ›æ–°å›é¡¾

vLLM é€šè¿‡ä¸‰ä¸ªå±‚æ¬¡çš„åˆ›æ–°è§£å†³äº† LLM æ¨ç†çš„å†…å­˜ç“¶é¢ˆï¼š

**1. ç®—æ³•å±‚ (PagedAttention)**:
- å°† Attention è®¡ç®—åˆ†è§£ä¸ºå—çº§æ“ä½œ
- æ”¯æŒéè¿ç»­å†…å­˜è®¿é—®
- æ•°å­¦ç­‰ä»·æ€§ä¿è¯

**2. ç³»ç»Ÿå±‚ (Memory Management)**:
- è™šæ‹Ÿå†…å­˜æŠ½è±¡
- æŒ‰éœ€åˆ†é…ï¼ŒåŠ¨æ€å¢é•¿
- Copy-on-Write å…±äº«æœºåˆ¶

**3. å®ç°å±‚ (GPU Kernels)**:
- èåˆå†…å­˜æ“ä½œå’Œè®¡ç®—
- ä¼˜åŒ–å†…å­˜è®¿é—®æ¨¡å¼
- æ‰¹å¤„ç†å‡å°‘å¼€é”€

### 8.2 å…³é”®æ•°æ®æ€»ç»“

| æŒ‡æ ‡ | æ”¹è¿›å¹…åº¦ |
|-----|---------|
| å†…å­˜åˆ©ç”¨ç‡ | 20% â†’ 96% (**4.8Ã—**) |
| ååé‡ (å•åºåˆ—) | **2-4Ã— vs Orca**, **6-22Ã— vs FasterTransformer** |
| ååé‡ (Beam Search) | **2.3Ã— vs Orca (Oracle)** |
| å†…å­˜èŠ‚çœ (Beam Search) | **55-66%** |
| Batch Size | **4-19Ã— more requests** |

### 8.3 è®¾è®¡å“²å­¦

vLLM çš„æˆåŠŸæºäºå‡ ä¸ªæ ¸å¿ƒè®¾è®¡åŸåˆ™ï¼š

1. **è·¨é¢†åŸŸè¿ç§»** (Cross-domain Transfer)
   - å€Ÿé‰´æˆç†Ÿçš„ OS æŠ€æœ¯
   - é€‚åº” AI ç³»ç»Ÿçš„ç‰¹ç‚¹

2. **åˆ†ç¦»å…³æ³¨ç‚¹** (Separation of Concerns)
   - é€»è¾‘è§†å›¾ vs ç‰©ç†å¸ƒå±€
   - ç¼–ç¨‹æ¥å£ vs å†…å­˜ç®¡ç†

3. **æ€§èƒ½ä¸é€šç”¨æ€§å¹³è¡¡** (Performance-Generality Tradeoff)
   - æ”¯æŒå¤šç§è§£ç ç®—æ³•
   - ä¿æŒé«˜æ€§èƒ½

4. **å®ç”¨ä¸»ä¹‰** (Pragmatism)
   - æ¥å— 20-26% çš„ kernel å¼€é”€
   - æ¢å– 2-4Ã— çš„ç«¯åˆ°ç«¯æå‡

### 8.4 å¯¹ AI Infra çš„å¯ç¤º

**1. å†…å­˜ç®¡ç†æ˜¯å…³é”®ç“¶é¢ˆ**:
- GPU è®¡ç®—èƒ½åŠ› > å†…å­˜å¸¦å®½ > å†…å­˜å®¹é‡
- ä¼˜åŒ–å†…å­˜ç®¡ç†æ¯”ä¼˜åŒ–è®¡ç®—æ›´é‡è¦

**2. OS æŠ€æœ¯å¯ä»¥æœ‰æ•ˆè¿ç§»**:
- è™šæ‹Ÿå†…å­˜ã€åˆ†é¡µã€CoWã€Swapping
- 60 å¹´çš„ç³»ç»Ÿç ”ç©¶æˆæœå¯ä»¥åº”ç”¨

**3. ç³»ç»Ÿä¼˜åŒ–éœ€è¦å…¨æ ˆæ€ç»´**:
- ç®—æ³• + ç³»ç»Ÿ + ç¡¬ä»¶
- ç«¯åˆ°ç«¯çš„æ€§èƒ½åˆ†æ

**4. å¼€æºæ˜¯æ¨åŠ¨åˆ›æ–°çš„å…³é”®**:
- vLLM çš„å¿«é€Ÿè¿­ä»£
- ç¤¾åŒºçš„å¹¿æ³›é‡‡ç”¨å’Œæ”¹è¿›

---

## ä¹ã€å‚è€ƒèµ„æº

### è®ºæ–‡

- **åŸå§‹è®ºæ–‡**: Kwon et al., "Efficient Memory Management for Large Language Model Serving with PagedAttention", SOSP 2023
- **ArXiv**: https://arxiv.org/abs/2309.06180

### å¼€æºä»£ç 

- **å®˜æ–¹ä»“åº“**: https://github.com/vllm-project/vllm
- **æ–‡æ¡£**: https://docs.vllm.ai/

### ç›¸å…³å·¥ä½œ

**LLM Serving Systems**:
1. Orca (OSDI 2022): Iteration-level scheduling
2. FasterTransformer (NVIDIA): Kernel optimizations
3. FlexGen (ICML 2023): Offloading for limited memory
4. AlpaServe (OSDI 2023): Statistical multiplexing

**Memory Optimizations**:
1. FlashAttention (NeurIPS 2022): IO-aware attention
2. ZeRO-Offload (ATC 2021): Training memory optimization
3. Megatron-LM (SC 2019): Model parallelism

**System Techniques**:
1. Virtual Memory (Kilburn et al., 1962): Original OS paper
2. Copy-on-Write: UNIX fork() optimization

---

## é™„å½•ï¼šå…³é”®ä»£ç ç‰‡æ®µ

### A. PagedAttention æ ¸å¿ƒç®—æ³•

```python
def paged_attention(
    query: Tensor,           # [batch, heads, d_head]
    block_tables: List[List[int]],  # [batch][num_blocks]
    context_lens: List[int], # [batch]
    block_size: int,
    num_kv_heads: int,
) -> Tensor:
    """
    PagedAttention çš„ç®€åŒ–å®ç°

    å‚æ•°:
        query: æŸ¥è¯¢å‘é‡
        block_tables: æ¯ä¸ªåºåˆ—çš„ block table
        context_lens: æ¯ä¸ªåºåˆ—çš„ä¸Šä¸‹æ–‡é•¿åº¦
        block_size: æ¯ä¸ªå—çš„å¤§å°
        num_kv_heads: KV å¤´çš„æ•°é‡

    è¿”å›:
        output: attention è¾“å‡º
    """
    batch_size = len(block_tables)
    num_query_heads = query.shape[1]
    head_dim = query.shape[2]

    output = torch.zeros_like(query)

    for i in range(batch_size):
        # å¤„ç†ç¬¬ i ä¸ªåºåˆ—
        num_blocks = (context_lens[i] + block_size - 1) // block_size

        # ç´¯åŠ  attention ç»“æœ
        attn_sum = torch.zeros(num_query_heads, head_dim)
        exp_sum = torch.zeros(num_query_heads)

        for block_idx in range(num_blocks):
            # è·å–ç‰©ç†å— ID
            physical_block = block_tables[i][block_idx]

            # è¯»å–å—æ•°æ®
            k_block = load_key_block(physical_block, num_kv_heads, block_size, head_dim)
            v_block = load_value_block(physical_block, num_kv_heads, block_size, head_dim)

            # è®¡ç®— attention scores (å—çº§)
            scores = torch.einsum('qhd,khd->qhk', query[i], k_block) / math.sqrt(head_dim)
            scores = torch.exp(scores)  # [num_query_heads, num_kv_heads, block_size]

            # ç´¯åŠ 
            exp_sum += scores.sum(dim=-1)  # [num_query_heads, num_kv_heads]
            attn_sum += torch.einsum('qhk,khd->qhd', scores, v_block)

        # å½’ä¸€åŒ–
        output[i] = attn_sum / exp_sum.unsqueeze(-1)

    return output
```

### B. Block Allocator

```python
class BlockAllocator:
    """ç®¡ç†ç‰©ç† KV å—çš„åˆ†é…å™¨"""

    def __init__(self, num_blocks: int, block_size: int):
        self.num_blocks = num_blocks
        self.block_size = block_size
        self.free_blocks = list(range(num_blocks))  # ç©ºé—²å—åˆ—è¡¨
        self.ref_counts = [0] * num_blocks           # å¼•ç”¨è®¡æ•°

    def allocate(self, num_blocks: int) -> List[int]:
        """åˆ†é…æŒ‡å®šæ•°é‡çš„å—"""
        if len(self.free_blocks) < num_blocks:
            raise OutOfMemoryError(f"Cannot allocate {num_blocks} blocks")

        allocated = []
        for _ in range(num_blocks):
            block_id = self.free_blocks.pop()
            self.ref_counts[block_id] = 1
            allocated.append(block_id)

        return allocated

    def free(self, block_ids: List[int]):
        """é‡Šæ”¾å—ï¼ˆå‡å°‘å¼•ç”¨è®¡æ•°ï¼‰"""
        for block_id in block_ids:
            assert self.ref_counts[block_id] > 0
            self.ref_counts[block_id] -= 1

            if self.ref_counts[block_id] == 0:
                self.free_blocks.append(block_id)

    def increase_ref(self, block_ids: List[int]):
        """å¢åŠ å¼•ç”¨è®¡æ•°ï¼ˆç”¨äºå…±äº«ï¼‰"""
        for block_id in block_ids:
            self.ref_counts[block_id] += 1

    def get_num_free_blocks(self) -> int:
        """è·å–ç©ºé—²å—æ•°é‡"""
        return len(self.free_blocks)
```

### C. Sequence Manager

```python
class SequenceManager:
    """ç®¡ç†åºåˆ—å’Œå®ƒä»¬çš„ block tables"""

    def __init__(self, allocator: BlockAllocator):
        self.allocator = allocator
        self.block_tables: Dict[int, List[int]] = {}
        self.next_seq_id = 0

    def create_sequence(self, prompt_len: int) -> int:
        """åˆ›å»ºæ–°åºåˆ—"""
        seq_id = self.next_seq_id
        self.next_seq_id += 1

        # åˆ†é… prompt éœ€è¦çš„å—
        num_blocks = (prompt_len + self.allocator.block_size - 1) // self.allocator.block_size
        blocks = self.allocator.allocate(num_blocks)

        self.block_tables[seq_id] = blocks
        return seq_id

    def fork_sequence(self, parent_id: int) -> int:
        """Fork åºåˆ—ï¼ˆç”¨äº parallel sampling / beam searchï¼‰"""
        if parent_id not in self.block_tables:
            raise ValueError(f"Sequence {parent_id} does not exist")

        # åˆ›å»ºå­åºåˆ—
        child_id = self.next_seq_id
        self.next_seq_id += 1

        # å…±äº«çˆ¶åºåˆ—çš„å—
        parent_blocks = self.block_tables[parent_id]
        self.block_tables[child_id] = parent_blocks.copy()

        # å¢åŠ å¼•ç”¨è®¡æ•°
        self.allocator.increase_ref(parent_blocks)

        return child_id

    def append_token(self, seq_id: int) -> Optional[int]:
        """
        ä¸ºåºåˆ—æ·»åŠ æ–° token
        è¿”å›æ–°åˆ†é…çš„å— IDï¼ˆå¦‚æœæœ‰ï¼‰
        """
        if seq_id not in self.block_tables:
            raise ValueError(f"Sequence {seq_id} does not exist")

        blocks = self.block_tables[seq_id]
        last_block = blocks[-1]

        # æ£€æŸ¥æœ€åä¸€ä¸ªå—æ˜¯å¦å·²æ»¡
        # ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…éœ€è¦ç»´æŠ¤æ¯ä¸ªå—çš„å¡«å……çŠ¶æ€ï¼‰
        if self._is_block_full(last_block):
            # Copy-on-Writeï¼ˆå¦‚æœéœ€è¦ï¼‰
            if self.allocator.ref_counts[last_block] > 1:
                new_block = self.allocator.allocate(1)[0]
                self._copy_block(last_block, new_block)

                self.allocator.free([last_block])
                blocks[-1] = new_block

            # åˆ†é…æ–°å—
            new_block = self.allocator.allocate(1)[0]
            blocks.append(new_block)
            return new_block

        return None

    def delete_sequence(self, seq_id: int):
        """åˆ é™¤åºåˆ—"""
        if seq_id not in self.block_tables:
            return

        blocks = self.block_tables[seq_id]
        self.allocator.free(blocks)
        del self.block_tables[seq_id]
```

---

**ä¸€å¥è¯æ€»ç»“**: vLLM é€šè¿‡å°†æ“ä½œç³»ç»Ÿçš„è™šæ‹Ÿå†…å­˜å’Œåˆ†é¡µæŠ€æœ¯åº”ç”¨åˆ° LLM çš„ KV Cache ç®¡ç†ï¼Œå®ç°äº†è¿‘é›¶å†…å­˜æµªè´¹ï¼ˆ96.3% åˆ©ç”¨ç‡ï¼‰å’Œ 2-4 å€çš„ååé‡æå‡ï¼Œæˆä¸ºå¤§æ¨¡å‹æ¨ç†ç³»ç»Ÿçš„é‡Œç¨‹ç¢‘å¼å·¥ä½œã€‚

---

**ä½œè€…**: Claude (AI Assistant)
**æ—¥æœŸ**: 2025-10-14
**ç‰ˆæœ¬**: v1.0
